## 3.1 入力、出力、ターゲット

ニューラルネットワークは、入力 x に対して出力 y = f(x, w) を生成する関数として定義される。  
ここで w はネットワークの全てのパラメータ（重みやバイアス）をまとめたベクトルである。

学習の目的は、ネットワークの出力 y が、与えられた教師信号（ターゲット） t にできるだけ近づくように、  
パラメータ w を調整することである。

このとき、どの程度「近いか」「正確か」を数値的に評価するための関数が、  
コスト関数（損失関数） E(y, t) である。

一般に、コスト関数は非負の実数値を取り、  
その値が小さいほどモデルの性能が良い（すなわち、出力がターゲットに近い）ことを意味する。

多くのタスクにおいて、ネットワークの出力 y およびターゲット t は多次元ベクトルである。  
たとえば、

  
	•	回帰問題では y, t \in \mathbb{R}^n の連続値ベクトル、  

	•	分類問題では y, t \in [0,1]^n の確率分布ベクトル（ソフトマックス出力）  

などとして扱う。

学習のプロセスでは、与えられたデータ集合 \{(x_p, t_p)\}{p=1}^P に対して、  
それぞれのサンプルにおけるコストを計算し、その平均（または総和）を  

全体のコスト関数  

E{\text{total}} = \frac{1}{P} \sum_{p=1}^{P} E(f(x_p, w), t_p)  

として定義することが多い。

ネットワークの学習は、このコスト関数を最小化するように w を更新する最適化問題として定式化される。


## 3.2 上言誤差関数

上限誤差関数とは、  
すべての出力成分のうち、最も大きい誤差をコストとして測る関数です。  
つまり、「一番悪い誤差だけを見て、全体の性能を評価する」方法です。

  
定義

出力ベクトル y = (y_1, y_2, \dots, y_n) と  
ターゲットベクトル t = (t_1, t_2, \dots, t_n) に対して、  

上限誤差関数 E_{\infty} は次のように定義されます：

  
E_{\infty}(y, t) = \max_{i} |y_i - t_i|  

ここで：

  
	•	|y_i - t_i| は各要素の誤差の絶対値  

	•	その中の最大値を取る  


 意味と直感

  
	•	「全体の平均的な誤差」ではなく、  
**最悪の誤差（最大のズレ）**を重視して評価する。  

	•	したがって、出力の一部に大きな誤差があれば、それが全体の損失を支配する。  

	•	よって、最悪ケースを重視するシステム（安全性や品質保証など）に向く。  


## 3.3 L2誤差関数

L₂誤差関数は、出力とターゲットの差の二乗和（平方誤差）で距離を測る関数。  
これは連続関数に対しては積分形式で、離散データではユークリッド距離として定義されます

 数学的定義

  
(1) 連続的な場合  

入力 x \in [0,1]、ターゲット関数 \varphi : [0,1] \to \mathbb{R} が平方可積分であるとすると：

  
C(w, b) = \int_{0}^{1} (f_{w,b}(x) - \varphi(x))^2 dx  

これは出力 f_{w,b}(x) と目標関数 \varphi(x) のL₂ノルム距離を測る。

離散的な場合

ターゲットが n 個の点でのみ与えられているとき：

  
z_i = \varphi(x_i), \quad i = 1, 2, \dots, n  

このとき、対応する離散的コスト関数は：

  
C(w, b) = \sum_{i=1}^{n} (f_{w,b}(x_i) - z_i)^2 = \| f_{w,b}(x) - z \|_2^2  

つまり、出力ベクトル f_{w,b}(x) とターゲットベクトル z のユークリッド距離の二乗です。

幾何的解釈

固定された入力 x に対して、写像  
(w, b) \mapsto f_{w,b}(x) は \mathbb{R}^n 内の超曲面（hypersurface）を定義する。  
コスト関数 C(w,b) は、ターゲット点 z とその超曲面上の点の距離を表しており、  
最小値は z から超曲面への**直交射影点（orthogonal projection）**で達成される。

すなわち：

  
(w^, b^) = \arg\min_{w,b} C(w,b)  

が最適パラメータとなる。


## 平均二乗誤差(MSE)

概要

ニューラルネットワークの出力 Y = f_{w,b}(X) と、  
目標値（ターゲット）Z の差を測る際に、  
「平均二乗誤差（Mean Square Error; MSE）」が最も一般的に用いられる。

数式定義：

  
C(w, b) = E[(Y - Z)^2] = E[(f_{w,b}(X) - Z)^2]  

ここで E[\cdot] は**期待値（確率平均）**を表す。  
つまり、「出力とターゲットの差の二乗の平均」を最小化するように  
パラメータ w, b を学習する。


## 3.5 CrossEntropy

クロスエントロピーは、2つの確率分布 p(x) と q(x) の間の「距離」や「不一致度」を測るための指標です。  
特にニューラルネットワークの分類問題において、出力分布と正解分布の差を測る損失関数として広く使われます 

定義

確率分布 p(x) に対して、別の分布 q(x) を評価する場合、  
クロスエントロピー S(p, q) は次のように定義されます：

  
S(p, q) = -\int p(x) \ln q(x) \, dx  

これは「分布 p の立場から見た、分布 q の情報量の平均」を意味します。  
つまり、もし q が真の分布 p と違うほど、S(p, q) は大きくなります。

性質

  
	1.	Shannon エントロピー H(p) との関係  

H(p) = -\int p(x) \ln p(x) \, dx  

が成り立ち、次の不等式が示されます：

  
S(p, q) \geq H(p)  

等号は p = q のときのみ成立します ￼。

	2.	つまり、クロスエントロピーの最小値は H(p)（真の分布自身の情報量）であり、  
q が p に近いほど S(p, q) は小さくなります。

	3.	確率モデル学習の目的関数としての利用  

ニューラルネットワークでは、  
出力の確率モデル p_\theta(y|x) と、  
訓練分布 p_{X,Z}(x,z) とのクロスエントロピーを最小化する：

  
C(\theta) = S(p_{X,Z}, p_\theta(Z|X))  

これを最小にするパラメータ \theta^* が、最も訓練分布に一致するモデルを与えます ￼。

応用的な意味

  
	•	分類問題での損失関数として頻用される（出力分布と正解分布の距離を測る）。  

	•	一方、回帰問題では MSE（平均二乗誤差） が一般的。  

	•	S(p, q) - H(p) は後に導入される Kullback–Leibler ダイバージェンス に対応します。
